{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T16:42:53.647359Z",
     "start_time": "2017-03-02T19:42:53.613074+03:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Q-learning in the wild\n",
    "\n",
    "Here we use the qlearning agent from before on taxi env from openai gym.\n",
    "You will need to insert a few agent functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T16:42:54.745252Z",
     "start_time": "2017-03-02T19:42:54.690533+03:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-02 19:42:54,726] Making new env: Taxi-v2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T16:51:18.982241Z",
     "start_time": "2017-03-02T19:51:18.953625+03:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from qlearning import QLearningAgent\n",
    "\n",
    "agent = QLearningAgent(alpha=0.5,epsilon=0.25,discount=0.99,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T16:51:19.409149Z",
     "start_time": "2017-03-02T19:51:19.367564+03:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def play_and_train(env,agent,t_max=10**4):\n",
    "    \"\"\"This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = agent.getAction(s)\n",
    "        \n",
    "        next_s,r,done,_ = env.step(a)\n",
    "        \n",
    "        #<train (update) agent for state s>\n",
    "        agent.update(s, a, next_s, r)\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:break\n",
    "        \n",
    "    return total_reward\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T16:51:24.633771Z",
     "start_time": "2017-03-02T19:51:19.937486+03:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward -5.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f16917fda58>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n31vdkiSxNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16919532b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.22609551875220107"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env,agent))    \n",
    "    if i % 100 ==0:\n",
    "        clear_output(True)\n",
    "        agent.epsilon *= 0.9\n",
    "        print(\"mean reward\", np.mean(rewards[-100:]))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "        \n",
    "agent.epsilon\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 reducing epsilon\n",
    "\n",
    "Try decreasing agent epsilon over time to make him reach positive score.\n",
    "\n",
    "The straightforward way to do so is to reduce epsilon every N games:\n",
    "* either multiply agent.epsilon by a number less than 1 (e.g. 0.99)\n",
    "* or substract a small value until it reaches 0\n",
    "\n",
    "You can, of-course, devise other strategies.\n",
    "\n",
    "__The goal is to reach positive reward!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. SARSA (2 pts)\n",
    "\n",
    "```<Please go to sarsa.py and implement the missing lines in update method>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T17:06:16.643709Z",
     "start_time": "2017-03-02T20:06:16.601854+03:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (sarsa.py, line 122)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/home/i/EDU/Practical_RL/week3/sarsa.py\"\u001b[0;36m, line \u001b[0;32m122\u001b[0m\n\u001b[0;31m    reference_qvalue = <Your Code Here>\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sarsa import SarsaAgent\n",
    "agent = SarsaAgent(alpha=0.1,epsilon=0.25,discount=0.99,\n",
    "                       getLegalActions = lambda s: range(n_actions))\n",
    "#Note that SARSA will likely need smaller learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def play_and_train_sarsa(env,agent,t_max=10**4):\n",
    "    \"\"\"This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = #<get agent to pick action given state s>\n",
    "        \n",
    "        next_s,r,done,_ = env.step(a)\n",
    "        \n",
    "        #<train (update) agent for state s>\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:break\n",
    "        \n",
    "    return total_reward\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train_sarsa(env,agent))    \n",
    "    if i %100 ==0:\n",
    "        clear_output(True)\n",
    "        print \"mean reward\",np.mean(rewards[-100:])\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Continuous state space (2 pts)\n",
    "\n",
    "Use agent to train on CartPole-v0\n",
    "\n",
    "This environment has a continuous number of states, so you will have to group them into bins somehow.\n",
    "\n",
    "The simplest way is to use `round(x,n_digits)` (or numpy round) to round real number to a given amount of digits.\n",
    "\n",
    "The tricky part is to get the n_digits right for each state to train effectively.\n",
    "\n",
    "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"first state:%s\"%(env.reset()))\n",
    "plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Play a few games\n",
    "\n",
    "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for _ in range(1000):\n",
    "    all_states.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        s,r,done,_ = env.step(env.action_space.sample())\n",
    "        all_states.append(s)\n",
    "        if done:break\n",
    "            \n",
    "all_states = np.array(all_states)\n",
    "\n",
    "for obs_i in range(env.observation_space.shape[0]):\n",
    "    \n",
    "    plt.hist(all_states[:,obs_i],bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Binarize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "class Binarizer(ObservationWrapper):\n",
    "    \n",
    "    def _observation(self,state):    \n",
    "        \n",
    "        #state = <round state to some amount digits.>\n",
    "        #hint: you can do that with round(x,n_digits)\n",
    "        #you will need to pick a different n_digits for each dimension\n",
    "\n",
    "        return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env = Binarizer(gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for _ in range(1000):\n",
    "    all_states.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        s,r,done,_ = env.step(env.action_space.sample())\n",
    "        all_states.append(s)\n",
    "        if done:break\n",
    "            \n",
    "all_states = np.array(all_states)\n",
    "\n",
    "for obs_i in range(env.observation_space.shape[0]):\n",
    "    \n",
    "    plt.hist(all_states[:,obs_i],bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from qlearning import QLearningAgent\n",
    "agent = QLearningAgent(alpha=0.5,epsilon=0.25,discount=0.99,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env,agent))    \n",
    "    if i %100 ==0:\n",
    "        clear_output(True)\n",
    "        print \"mean reward\",np.mean(rewards[-100:])\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Homework\n",
    "\n",
    "## 4. Expected value SARSA (2 pts)\n",
    "\n",
    "```<go to expected_value_sarsa.py and implement missing lines in getValue(state)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from expected_value_sarsa import EVSarsaAgent\n",
    "agent = EVSarsaAgent(alpha=0.5,epsilon=0.25,discount=0.99,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train EV-SARSA\n",
    "\n",
    "Note that it uses __the same update parameters as__ qlearning so you can use the ```play_and_train``` function from q-learning.\n",
    "\n",
    "Please try both constant epsilon = 0.25 and decreasing epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.2 EV-sarsa on CartPole\n",
    "\n",
    "Now train the `EVSarsaAgent` on CartPole-v0 env with binarizer you used above for Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env = <make env and wrap it with binarizer>\n",
    "\n",
    "agent = <your code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "<train me>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Massive experiments\n",
    "\n",
    "This is the final part of the homework. You can pick any of the 3 tasks listed below. Or take more that one and get score for each of them independently.\n",
    "\n",
    "_If you feel to cool for this kind of school, see bonus section below - it awwards just as much points_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Algorithm comparison (4 points)\n",
    "\n",
    "\n",
    "For this experiment, you will need to write the code to answer to compare algorithm performance and produce plots/tables with experimental results that can be used to compare them.\n",
    "\n",
    "Take CartPole or Taxi and compare learning performance of all 3 algorithms under those conditions:\n",
    "\n",
    "* Constant epsilon 0.25, 0.1 and 0.001\n",
    "* Decreasing epsilong starting from 0.25 (decrease any way you want)\n",
    "* It's probably a good idea to plot learning curves (reward / games played)\n",
    "* At the end of your assignment, please describe in which conditions does each algorithm work better (if at all).\n",
    "\n",
    "* It's also useful to double-check if experiment results are robust to re-running and if they aren't - average over several runs.\n",
    "* If you use CartPole-v0, use same binarization techniques.\n",
    "\n",
    "It is __highly recommended__ that your code automatically builds the plot or prints the table.\n",
    "\n",
    "A creative approach to visualization or trying out more ideas will be awwarded with bonus points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bonus I: Advanced algorithms (4+ points)\n",
    "\n",
    "Implement any of the three algorithms:\n",
    "* n-step expected value SARSA or Q-learning\n",
    "* EV-SARSA or Q-learning( using eligibility traces aka TD(lambda)\n",
    "* q-learning with experience replay\n",
    "\n",
    "_(you will likely need to create a new file for that, just like qlearning.py)_\n",
    "\n",
    "* Show that this algorithm works no worse than those we already implemented for simple problems. \n",
    "* Try to find a way to learn faster than with default q-learning.\n",
    "\n",
    "You will also get +2 points for each algorithm implemented after the first one and any other awesomeness you're up to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bonus II: Binarization techniques (4+ points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Measure how learning performance depends on binarization and try some advanced binarizations.\n",
    "\n",
    "On CartPole-v0,\n",
    "* Measure learning speed and final performance against changing the amount of bins (uniformly across all dimensions) __(1 point)__\n",
    "* Try pre-processing observation with PCA, SparseCoding or any dimensionality reduction method you want, see what happens __(1 point)__\n",
    "\n",
    "* Apply binarization to solve MountainCar-v0 or LunarLander-v2 __(+2 points each)__\n",
    "\n",
    "_Warning, Mountaincar-v0 and LunarLander-v2 may train for ~hour. The only sanity check is that the frequency of successes more or less increases._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bonus II+\n",
    "\n",
    "Try applying categorical deep autoencoder as a binarization technique.\n",
    "\n",
    "Use gumbel-softmax, \n",
    "* Explaination and [tutorial](http://blog.evjang.com/2016/11/tutorial-categorical-variational.html), \n",
    "* [Example in lasagne](https://gist.github.com/justheuristic/fd08c15dee26dbe95d3e3a17855f3f7a/)\n",
    "\n",
    "* If you make it work on Cartpole, it's +5. \n",
    "* If on LunarLander or MountainCar, it's +5 more.\n",
    "* If it somehow ends up good on Atari (see week1 homework) or BipedalWalker-v2 or anything serious, it's a full project ( more pts :) )\n",
    "* If you have any questions or need any help, feel free to ask us!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
